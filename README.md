# EXP–2 – PROMPT ENGINEERING
# Comparative Analysis of Different Types of Prompting Patterns and Explanation with Various Test Scenarios
## Aim

To compare different prompting patterns and evaluate how they influence the quality, accuracy, clarity, and depth of responses generated by AI models across various test scenarios.

## Experiment

In this experiment, different prompting styles (broad prompts, structured prompts, role-based prompts, chain-of-thought prompts, zero-shot prompts, and few-shot prompts) are tested on multiple scenarios.
Each type of prompt is given the same task to understand how structure and clarity affect the final output.

### The scenarios used include:

Fact Retrieval Scenario

Creative Writing Scenario

Analytical Reasoning Scenario

Decision-Making Scenario

### The generated responses are compared based on:

Relevance

Accuracy

Depth of explanation

Coherence

Creativity

## Algorithm

### Select Prompt Patterns

Broad/Unstructured Prompt

Structured Prompt

Role-Based Prompt

Chain-of-Thought Prompt

Few-Shot Prompt

Prepare Test Scenarios
(Fact-Based, Creative, Analytical, Decision-Making)

Apply Each Prompt Type to All Scenarios

Collect Outputs

Save responses

Note quality indicators

Analyze the Responses

Compare clarity, detail, relevance

Identify strengths and weaknesses of each pattern

Summarize Findings

Highlight which prompt type performs best in which scenario

## Output (Sample Observation Summary)
### 1. Broad Prompt

Responses were vague, incomplete, and lacked direction.

High creativity but low accuracy.

### 2. Structured Prompt

Produced organized and clear responses.

Best for technical or academic tasks.

### 3. Role-Based Prompt

Produced authoritative and context-rich answers.

Useful for professional writing.

### 4. Chain-of-Thought Prompt

Provided deeper reasoning and step-by-step logic.

Best suited for problem-solving or analysis.

### 5. Few-Shot Prompt

Showed patterns from examples.

Gave more consistent and aligned outputs.



## Picture (Based on the Experiment — NOT containing the content)

Here is a simple conceptual diagram showing comparison of prompting patterns:

Prompt Types Comparison Diagram
 ┌─────────────────────────────┐
 │        Prompt Types         │
 ├─────────────────────────────┤
 │ • Broad Prompt              │
 │ • Structured Prompt         │
 │ • Role-Based Prompt         │
 │ • Chain-of-Thought Prompt   │
 │ • Few-Shot Prompt           │
 └──────────────┬──────────────┘
                │
                ▼
      ┌───────────────────────┐
      │     Test Scenarios    │
      │  (Creative, Logical,  │
      │   Factual, Decision)  │
      └──────────────┬────────┘
                     │
                     ▼
        ┌────────────────────────┐
        │  Output Comparison      │
        │  (Accuracy, Depth,      │
        │   Clarity, Relevance)   │
        └────────────────────────┘

## Result

The study concludes that structured prompting greatly improves accuracy, clarity, and analytical depth of an AI-generated response.
While broad prompts encourage creativity, they lack precision.
Chain-of-thought and role-based prompts perform best in reasoning and professional tasks.
Few-shot prompts give consistent pattern-aligned results.

Thus, selecting the right prompting technique significantly influences output quality across different scenarios.
